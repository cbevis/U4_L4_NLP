{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'. \n",
    "    text = re.sub(r'--',' ',text)\n",
    "    # get rid of text between brackets\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "milton = gutenberg.raw('milton-paradise.txt')\n",
    "chesterton = gutenberg.raw('chesterton-brown.txt')\n",
    "\n",
    "# Chapters are book specific\n",
    "# Need to clear chapters before runnign clean text\n",
    "chesterton = re.sub('[IVX][IVX]*\\. *The.*', '', chesterton)\n",
    "milton = re.sub(r'Book .*', '', milton)\n",
    "\n",
    "milton_clean = text_cleaner(milton)\n",
    "chesterton_clean = text_cleaner(chesterton)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. \n",
    "nlp = spacy.load('en')\n",
    "chesterton_doc = nlp(chesterton_clean)\n",
    "milton_doc = nlp(milton_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. \n",
    "chesterton_doc = nlp(chesterton_clean)\n",
    "milton_doc = nlp(milton_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into sentences\n",
    "chesterton_sents = [[sent, 'Chesterton'] for sent in chesterton_doc.sents]\n",
    "milton_sents = [[sent, 'Milton'] for sent in milton_doc.sents]\n",
    "\n",
    "# Combine the setnences from the two novels into one data frame\n",
    "sentences = pd.DataFrame(chesterton_sents + milton_sents)\n",
    "sentences.columns = ['text_sentences', 'author']\n",
    "sentences.head()\n",
    "\n",
    "both_sents = [' '.join([token.lemma_ for token in sent]) for sent in sentences['text_sentences']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the consulting - room of dr orion hood , the eminent criminologist and specialist in certain moral disorder , lie along the sea - front at scarborough , in a series of very large and well - light french window , which show the north sea like one endless outer wall of blue - green marble .',\n",
       " 'in such a place the sea have something of the monotony of a blue - green dado : for the chamber -PRON- be rule throughout by a terrible tidiness not unlike the terrible tidiness of the sea .',\n",
       " \"-PRON- must not be suppose that dr hood 's apartment exclude luxury , or even poetry .\",\n",
       " 'these thing be there , in -PRON- place ; but one feel that -PRON- be never allow out of -PRON- place .',\n",
       " 'luxury be there : there stand upon a special table eight or ten box of the good cigar ; but -PRON- be build upon a plan so that the strong be always nearest the wall and the mildest nearest the window .']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_sents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add function to make stop words case insensitive\n",
    "stop_words_getter = lambda token: token.is_stop or token.lower_ in STOP_WORDS \n",
    "Token.set_extension('is_stop', getter=stop_words_getter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token._.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token._.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the bags.\n",
    "milton_words = bag_of_words(milton_doc)\n",
    "chesterton_words = bag_of_words(chesterton_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(milton_words + chesterton_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "#word_counts = bow_features(sentences, common_words)\n",
    "#word_counts.head()\n",
    "#word_counts.to_csv('word_counts_p5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_words(example_sentence):\n",
    "    return [token for token in example_sentence if not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['words'] = sentences['text_sentences'].apply(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'abruptly', 'absence', 'absent', 'abyss', 'accept', 'accord', 'account', 'accuse', 'achieve', 'act', 'action', 'actor', 'adam', 'add', 'address', 'admiral', 'admiration', 'admire', 'admit', 'adore', 'adorn', 'advance', 'advanced', 'advantage', 'advise', 'affair', 'afraid', 'age', 'ago', 'aid', 'air', 'alike', 'alive', 'allow', 'almighty', 'aloud', 'altar', 'alter', 'ambition', 'american', 'amid', 'amidst', 'ancient', 'angel', 'angelick', 'angels', 'anger', 'anon', 'answer', 'anybody', 'appear', 'appearance', 'appease', 'appetite', 'approach', 'arch', 'argue', 'arise', 'ark', 'arm', 'armagnac', 'armed', 'army', 'array', 'arrive', 'art', 'arthur', 'ascend', 'ask', 'aspect', 'aspire', 'assault', 'assent', 'assume', 'assure', 'astonishment', 'attain', 'attempt', 'attend', 'attention', 'audience', 'aught', 'aurora', 'author', 'away', 'awful', 'bad', 'bald', 'band', 'bank', 'banker', 'bar', 'bare', 'battle', 'beam', 'bear', 'beard', 'beast', 'beauty', 'bed', 'befall', 'begin', 'beheld', 'behold', 'believe', 'bell', 'belong', 'bend', 'beneath', 'bent', 'best', 'better', 'bid', 'big', 'bind', 'bird', 'birth', 'bit', 'black', 'blackmail', 'blade', 'blame', 'blaze', 'blest', 'blind', 'bliss', 'blood', 'blow', 'blue', 'boast', 'body', 'bold', 'book', 'bough', 'boulnois', 'bound', 'bow', 'bower', 'box', 'branch', 'break', 'breast', 'breath', 'breathe', 'breed', 'brigand', 'bright', 'bring', 'broad', 'broken', 'brother', 'brow', 'brown', 'bruise', 'brun', 'bruno', 'brute', 'build', 'bullet', 'burglar', 'burn', 'burst', 'business', 'butler', 'calm', 'canst', 'captain', 'care', 'carry', 'carstairs', 'case', 'cast', 'castle', 'catch', 'cause', 'cave', 'cease', 'celestial', 'center', 'certain', 'certainly', 'chain', 'chair', 'champion', 'chance', 'change', 'chaos', 'character', 'charge', 'chariot', 'charm', 'cherubim', 'chief', 'chiefly', 'child', 'choice', 'choose', 'church', 'cigar', 'circle', 'circuit', 'city', 'claim', 'claude', 'clear', 'clerical', 'clever', 'cliff', 'climb', 'close', 'clothe', 'cloud', 'cloudy', 'coast', 'coat', 'coin', 'cold', 'collection', 'colonel', 'colour', 'coloured', 'come', 'command', 'common', 'companion', 'company', 'comparatively', 'compare', 'complete', 'compose', 'conceal', 'conceive', 'concern', 'confess', 'confine', 'confirm', 'confound', 'confusion', 'connect', 'conscious', 'consult', 'contain', 'content', 'continue', 'converse', 'convict', 'cook', 'cool', 'corner', 'counsel', 'countenance', 'country', 'courage', 'courier', 'course', 'court', 'cover', 'cray', 'create', 'creation', 'creator', 'creature', 'creep', 'crest', 'crew', 'crime', 'criminal', 'crooked', 'cross', 'crowd', 'crown', 'cry', 'curiosity', 'curious', 'curse', 'curve', 'cut', 'cutlass', 'cutler', 'dagger', 'daily', 'dance', 'danger', 'dangerous', 'dare', 'dark', 'darkness', 'dart', 'daughter', 'dawn', 'day', 'dead', 'deal', 'dear', 'death', 'deceive', 'declare', 'decree', 'deed', 'deem', 'deep', 'defence', 'defend', 'degree', 'deity', 'delay', 'delicious', 'delight', 'demand', 'depart', 'descend', 'describe', 'deserve', 'design', 'desire', 'despair', 'destroy', 'destruction', 'detective', 'devil', 'dew', 'didst', 'die', 'different', 'difficulty', 'dignity', 'dim', 'dinner', 'dire', 'direct', 'direction', 'discern', 'discourse', 'discover', 'dismal', 'dismiss', 'distance', 'distant', 'disturb', 'divide', 'divine', 'doctor', 'dog', 'dominion', 'doom', 'door', 'double', 'doubt', 'doubtful', 'dr', 'draw', 'dread', 'dreadful', 'dream', 'dress', 'dressing', 'drink', 'drive', 'drop', 'dry', 'dubosc', 'duke', 'dust', 'dwell', 'ear', 'early', 'earth', 'earthly', 'ease', 'easily', 'east', 'eastern', 'easy', 'eat', 'echo', 'eden', 'edge', 'effect', 'egypt', 'element', 'embrace', 'eminent', 'empire', 'empyreal', 'enclose', 'end', 'endless', 'endure', 'enemy', 'england', 'english', 'enjoy', 'enormous', 'ensue', 'enter', 'entertain', 'entire', 'entirely', 'entrance', 'envy', 'equal', 'equally', 'er', 'ere', 'erect', 'err', 'escape', 'especially', 'eternal', 'ethereal', 'eve', 'evening', 'event', 'evidence', 'evidently', 'evil', 'exactly', 'examine', 'excel', 'excitement', 'exclaim', 'excuse', 'exmoor', 'expect', 'explain', 'expose', 'express', 'expression', 'extend', 'extraordinary', 'eye', 'ezza', 'fable', 'face', 'fact', 'fail', 'faint', 'fair', 'fairy', 'faith', 'faithful', 'falconroy', 'fall', 'false', 'fame', 'family', 'fancy', 'fanshaw', 'far', 'fast', 'fatal', 'fate', 'father', 'fault', 'favour', 'fear', 'feature', 'feed', 'feel', 'feign', 'fellow', 'fence', 'field', 'fiend', 'fierce', 'fiery', 'fight', 'figure', 'fill', 'final', 'find', 'finger', 'finish', 'finn', 'fire', 'firm', 'firmament', 'fish', 'fit', 'fix', 'flambeau', 'flame', 'flaming', 'flash', 'flat', 'flee', 'flesh', 'flight', 'fling', 'float', 'flock', 'flood', 'flow', 'flower', 'flowery', 'fly', 'foe', 'follow', 'fond', 'food', 'foot', 'forbid', 'force', 'foreign', 'forest', 'forget', 'form', 'forth', 'forthwith', 'forward', 'foul', 'fountain', 'fowl', 'frame', 'fraud', 'free', 'freedom', 'freely', 'french', 'fresh', 'friend', 'fro', 'frown', 'fruit', 'fulfil', 'fury', 'future', 'gabriel', 'gain', 'garden', 'gate', 'gather', 'gay', 'gaze', 'general', 'gentle', 'gentleman', 'gently', 'german', 'gesture', 'giant', 'gift', 'girl', 'glad', 'glance', 'glass', 'globe', 'glorious', 'glory', 'glow', 'god', 'goddess', 'gods', 'gold', 'golden', 'good', 'goodness', 'grace', 'grand', 'grass', 'grateful', 'grave', 'gravely', 'great', 'green', 'grey', 'groan', 'ground', 'grove', 'grow', 'guard', 'guess', 'guest', 'guide', 'guile', 'gulf', 'gun', 'hail', 'hair', 'haired', 'half', 'hall', 'hand', 'handsome', 'hang', 'happen', 'happiness', 'happy', 'hard', 'hardly', 'harm', 'harp', 'harrogate', 'hast', 'haste', 'hat', 'hate', 'hath', 'head', 'heap', 'hear', 'heart', 'heat', 'heaven', 'heavenly', 'heavens', 'heavy', 'hedge', 'heel', 'height', 'hell', 'help', 'henceforth', 'herb', 'herd', 'hide', 'hideous', 'high', 'highly', 'highth', 'hill', 'hirsch', 'hiss', 'hither', 'hold', 'hole', 'hollow', 'holy', 'home', 'honour', 'hood', 'hop', 'hope', 'horn', 'horrible', 'horrid', 'horror', 'horse', 'host', 'hot', 'hotel', 'hour', 'house', 'hue', 'huge', 'human', 'humble', 'humour', 'hurt', 'husband', 'idea', 'ill', 'image', 'imagine', 'immediately', 'immortal', 'impose', 'impression', 'incense', 'indian', 'infernal', 'infinite', 'inflame', 'influence', 'innocence', 'innocent', 'innumerable', 'inquire', 'inside', 'inspire', 'instant', 'instantly', 'instead', 'intend', 'interest', 'interrupt', 'interval', 'invisible', 'inward', 'ire', 'iron', 'island', 'issue', 'italian', 'james', 'john', 'join', 'journalist', 'journey', 'jove', 'joy', 'judge', 'judgement', 'jump', 'justice', 'kidd', 'kill', 'kind', 'king', 'kingdom', 'knee', 'knot', 'know', 'knowledge', 'labour', 'lady', 'lake', 'lamp', 'land', 'large', 'late', 'laugh', 'laughter', 'law', 'lay', 'lead', 'leader', 'leaf', 'lean', 'learn', 'leave', 'left', 'leg', 'legend', 'legion', 'length', 'lest', 'let', 'letter', 'level', 'liberty', 'librarian', 'lie', 'life', 'lift', 'light', 'like', 'likely', 'limb', 'line', 'link', 'lip', 'liquid', 'listen', 'little', 'live', 'living', 'lock', 'lodge', 'long', 'longer', 'look', 'loose', 'lord', 'lose', 'loss', 'lot', 'loud', 'love', 'lovely', 'low', 'luxury', 'machine', 'macnab', 'mad', 'main', 'major', 'maker', 'malice', 'man', 'mankind', 'manner', 'march', 'mark', 'marry', 'mask', 'matter', 'mean', 'measure', 'meek', 'meet', 'memory', 'men', 'mercy', 'mere', 'merely', 'merit', 'messiah', 'michael', 'mid', 'middle', 'midst', 'mighty', 'mild', 'mile', 'military', 'million', 'mind', 'minute', 'miserable', 'misery', 'miss', 'mist', 'mistake', 'mix', 'moment', 'money', 'monkey', 'monster', 'monstrous', 'moon', 'morn', 'morning', 'mortal', 'mother', 'motion', 'mould', 'mount', 'mountain', 'mouth', 'movement', 'mr', 'mrs', 'multiply', 'multitude', 'murder', 'muscari', 'mute', 'mysterious', 'mystery', 'naked', 'narrow', 'nation', 'native', 'natural', 'nature', 'near', 'nearer', 'nearly', 'neck', 'need', 'new', 'nigger', 'nigh', 'night', 'noise', 'noon', 'north', 'nose', 'note', 'notice', 'notion', 'number', 'numerous', 'obedience', 'obey', 'object', 'obscure', 'observe', 'obtain', 'obvious', 'ocean', 'odd', 'offend', 'offer', 'office', 'officer', 'official', 'offspring', 'oft', 'oh', 'old', 'omnipotent', 'open', 'opening', 'oppose', 'opposite', 'orb', 'ordain', 'order', 'orient', 'original', 'otto', 'outline', 'outside', 'outward', 'overcome', 'owe', 'pace', 'pain', 'pair', 'palace', 'pale', 'paper', 'paradise', 'pardon', 'parent', 'park', 'parkinson', 'party', 'pass', 'passage', 'passion', 'past', 'path', 'patience', 'pause', 'pay', 'peace', 'peculiar', 'peer', 'pendragon', 'people', 'perfect', 'perfectly', 'perform', 'permit', 'perpetual', 'person', 'philip', 'pick', 'pilgrim', 'pine', 'pink', 'pity', 'place', 'plain', 'plant', 'play', 'pleasant', 'pleased', 'pleasure', 'pluck', 'plunge', 'pocket', 'poet', 'point', 'poison', 'police', 'pond', 'pool', 'pooley', 'poor', 'popular', 'port', 'possess', 'possible', 'pour', 'power', 'powerful', 'powers', 'praise', 'prayer', 'prefer', 'prepare', 'presence', 'present', 'presume', 'pretty', 'prey', 'pride', 'priest', 'prime', 'prince', 'prison', 'private', 'probably', 'proceed', 'produce', 'promise', 'pron', 'pronounce', 'proof', 'prospect', 'proud', 'prove', 'public', 'pull', 'punishment', 'pure', 'purple', 'purpose', 'pursue', 'putnam', 'quarrel', 'queer', 'question', 'quick', 'quietly', 'race', 'radiant', 'rag', 'rage', 'rain', 'raise', 'range', 'rank', 'ransom', 'ray', 'reach', 'read', 'ready', 'real', 'realize', 'realm', 'rear', 'reason', 'rebel', 'recall', 'receive', 'recover', 'red', 'reduce', 'refuse', 'regard', 'region', 'reign', 'rejoice', 'relate', 'remain', 'remember', 'remote', 'remove', 'renew', 'repair', 'repeat', 'repent', 'reply', 'require', 'resolve', 'rest', 'resume', 'retain', 'retire', 'return', 'reveal', 'revenge', 'reverence', 'revive', 'revolt', 'rich', 'ride', 'ridge', 'right', 'ring', 'rise', 'rite', 'river', 'road', 'rock', 'roll', 'roman', 'romantic', 'rome', 'room', 'root', 'rope', 'rose', 'round', 'row', 'ruin', 'rule', 'run', 'rush', 'sacred', 'sad', 'safe', 'sail', 'saint', 'sake', 'sand', 'satan', 'savage', 'save', 'scale', 'scarce', 'scarf', 'scatter', 'scene', 'science', 'scientific', 'scorn', 'se', 'sea', 'seal', 'search', 'season', 'seat', 'second', 'secret', 'secure', 'seduce', 'seed', 'seek', 'seize', 'self', 'send', 'sense', 'sentence', 'seriously', 'serpent', 'servant', 'serve', 'service', 'set', 'seven', 'severe', 'seymour', 'shade', 'shadow', 'shady', 'shake', 'shall', 'shalt', 'shame', 'shape', 'sharp', 'sharply', 'shed', 'shield', 'shin', 'shine', 'ship', 'shock', 'shone', 'shoot', 'shore', 'short', 'shot', 'shoulder', 'shout', 'shower', 'shun', 'shut', 'sigh', 'sight', 'sign', 'signal', 'silence', 'silent', 'silk', 'silver', 'simple', 'simplicity', 'simply', 'sin', 'sing', 'single', 'sink', 'sir', 'sire', 'sit', 'sky', 'sleep', 'slight', 'slightly', 'slip', 'slow', 'slowly', 'small', 'smell', 'smile', 'smoke', 'smooth', 'snow', 'society', 'soft', 'soil', 'soldier', 'sole', 'solemn', 'solid', 'solitude', 'somewhat', 'son', 'song', 'soon', 'sorrow', 'sort', 'soul', 'sound', 'south', 'sovran', 'space', 'spacious', 'spake', 'speak', 'spear', 'specially', 'speech', 'speed', 'spend', 'sphere', 'spirit', 'spite', 'spot', 'spread', 'spring', 'spy', 'square', 'st', 'stand', 'star', 'starry', 'start', 'state', 'stay', 'steadily', 'steady', 'steal', 'steep', 'step', 'stick', 'stone', 'stop', 'store', 'storm', 'story', 'straight', 'strange', 'stranger', 'stream', 'street', 'strength', 'stretch', 'strict', 'stride', 'strike', 'stroke', 'strong', 'struggle', 'study', 'style', 'subdue', 'subject', 'sublime', 'substance', 'subtle', 'success', 'sudden', 'suddenly', 'suffer', 'sufficient', 'suggest', 'sum', 'summer', 'summon', 'sun', 'sunset', 'superiour', 'support', 'suppose', 'supreme', 'sure', 'surprise', 'surprised', 'sustain', 'sway', 'swear', 'sweet', 'swift', 'sword', 'table', 'tale', 'talk', 'tall', 'taste', 'teach', 'tear', 'tell', 'temper', 'temple', 'tempt', 'tend', 'tent', 'terrible', 'terrour', 'th', 'thank', 'theatre', 'thee', 'theory', 'thick', 'thieves', 'thin', 'thine', 'thing', 'think', 'thither', 'thou', 'thought', 'thousand', 'threaten', 'thrice', 'throne', 'thrones', 'throng', 'throw', 'thunder', 'thy', 'thyself', 'till', 'time', 'title', 'todd', 'todhunter', 'tone', 'tongue', 'torment', 'touch', 'tower', 'town', 'trace', 'track', 'train', 'transgress', 'travel', 'tree', 'trial', 'trick', 'triumph', 'trouble', 'true', 'trumpet', 'trust', 'truth', 'try', 'turn', 'twilight', 'understand', 'universal', 'unknown', 'unseen', 'upper', 'upright', 'urge', 'use', 'usher', 'usual', 'utmost', 'utter', 'vain', 'valley', 'vanish', 'vapour', 'vast', 'veil', 'vengeance', 'vessel', 'victor', 'view', 'violence', 'violent', 'virgin', 'virtue', 'visage', 'vision', 'visit', 'voice', 'void', 'wait', 'wake', 'walk', 'wall', 'wander', 'want', 'war', 'warm', 'warn', 'waste', 'watch', 'water', 'watery', 'wave', 'way', 'weak', 'weapon', 'wear', 'weight', 'west', 'western', 'whatev', 'wheel', 'wherefore', 'whereof', 'whereon', 'white', 'wicked', 'wide', 'wife', 'wig', 'wild', 'wilderness', 'wilson', 'win', 'wind', 'window', 'wine', 'wing', 'wisdom', 'wise', 'wish', 'witness', 'woe', 'woman', 'womb', 'wonder', 'wonderous', 'wood', 'word', 'work', 'world', 'worship', 'worth', 'worthy', 'wound', 'wrath', 'write', 'wrong', 'yard', 'ye', 'year', 'yellow', 'yes', 'yield', 'yonder', 'young', 'youth']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=2000, min_df=10, stop_words=STOP_WORDS)\n",
    "X = vectorizer.fit_transform(both_sents)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6465x1429 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 50961 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9902036607373035\n",
      "\n",
      "Test set score: 0.8731631863882444\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "Y = sentences['author']\n",
    "X_array = X.toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    Y, \n",
    "                                                    test_size=0.4, \n",
    "                                                    random_state=0)\n",
    "\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = rfc.feature_importances_\n",
    "importances = -np.sort(-importances)[:10]\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(10), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-e2d4bbebd5d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Applying the vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msents_tfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of features: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msents_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(sentences, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "sents_tfidf=vectorizer.fit_transform(sentences[0])\n",
    "print(\"Number of features: %d\" % sents_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(sents_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
